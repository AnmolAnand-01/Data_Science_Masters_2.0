{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e4ed20-1f75-42c8-a7fa-b9d759887f06",
   "metadata": {},
   "source": [
    "# Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf6b29-614a-41ad-92c5-136d8870a5c1",
   "metadata": {},
   "source": [
    "Web scraping refers to the process of extracting information or data from websites. It involves using automated tools and scripts to collect and gather data from web pages by sending HTTP requests to web servers and parsing HTML or other structured data in the web Pages.\n",
    "\n",
    "Web scraping is used for various purposes:-\n",
    "\n",
    "1. Data collection and analysis: Web scraping is used to collect vast amount of data from websites for analysis, research or informational purpose. Companies use web scraping for pricing information, product details, review and other information related to their business.\n",
    "\n",
    "2. Content Aggregation and Monitoring: Media companies, news aggregator and content cteators use web scraping to collect information from different sources across the web. This enables them to curate information, create content, or monitor changes and updates on specific topic or industries.\n",
    "\n",
    "3. Financial and Business intelligence: Financial institutions and analysts usesweb scraping to collect data on stock price, market fluctuations, economic indicators and other finance related informations. This data aids in making investment decisions and perform markst ananlysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ab554d-d0bf-4cba-ba9d-e3df5d9cdf51",
   "metadata": {},
   "source": [
    "# Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b8c3c-41ce-44bb-82b1-0e5ba9606bfb",
   "metadata": {},
   "source": [
    "Web scraping involves extracting data and information from websites. There are several methods for web scraping, some of them are listeed below:\n",
    "\n",
    "1. Python offers various frameworks and libraries for web scraping like, Beautiful Soup, Scrapy, Requests-HTML, selenium.\n",
    "\n",
    "2. Web scraping involves sending HTTP requests(GET/POST) to web servers to retrive HTML content. By utilising tools like cURL and libraries like Python's request library, developers can easily send HTTP requests to web servers to retrive HTML content. This content can then be parsed to extract relevent information.\n",
    "\n",
    "3. XPath and CSS selectors are methods to identify and extract specific information from HTML or XML documents. They allow for precise targeting of elements based on their structure, attributes or location within the document.\n",
    "\n",
    "4. Some websites provide APIs allowing controlled acces to their data, providing a structured and easier way to extract information. When available, using an API for Data extraction is usually more efficient and reliable compared to scraping HTML directly from their website.\n",
    "\n",
    "5. Headless Browsers are browser environment that operate without a graphical user interface(GUI). They can be used for web scraping purpose to simulate the behaviour of a standard web browser without displaying the content visually. In Python Selenium can be used as Headless Chrome or firefox for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb1327-a5cf-449d-8954-cda648aeb3a3",
   "metadata": {},
   "source": [
    "# Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6537c6-f933-4933-a2be-e7bd66b7e142",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library that is widely used for web scraping. It is specifically designed to parse HTML and XML allowing users to extract data from web page in a simple and effective manner. Beautiful Soup creates a parsed tree from parsed page, Which can be then easily traversed to find, extract and manipulate desired data.\n",
    "\n",
    "Reasons for using the Beautiful Soup Library:\n",
    "\n",
    "1. Beautiful soup provides a powerful way to navigate and seaarch through HTML/XML content, making it easy to extract specific element such as, links, headers, paragraph, table, etc, based on tags, attributes and text content.\n",
    "\n",
    "2. It offers user friendly interface, and simple syntax making it accesible even to user with minimum programming experience. It's intutive methods and functions facilitate easy extraction of information without writing complex code structures.\n",
    "\n",
    "3. Beautiful Soup is resilient to poorly formated or messy HTML code, commonly found across the web. It can handle imperfect HTML structures and still extract information effectively.\n",
    "\n",
    "4. Beautiful Soup supports various parser including Python's built-in parser, \"html.parser\", as well as external parser like \"lxml\" and \"html5lib\". This flexibility allows user to choose the parser that best suits their specific scraping needs in terms of speed, compitability and other requirements.\n",
    "\n",
    "5. It is compitable with Python2 and Python3 making it widely applicable across different python environments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc4a0fe-26e1-43b8-89bd-a4de6edc7e08",
   "metadata": {},
   "source": [
    "# Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6536ba08-2eee-4383-8f1e-4c2941ec21d7",
   "metadata": {},
   "source": [
    "Flask is a Python web framework used primarily for building Web applications. While flask was not directly involved in the process of web scraping, still it was used due to the following rwasons:\n",
    "\n",
    "1. Flask was used to create web interface to display scraped data. Oncse the data was collected through web scraping, Flask facilitated the presentation of this data on a web page.\n",
    "\n",
    "2. Flask was used to create an API(Application Programming Interface) that provides access to the scraped data.\n",
    "\n",
    "3. Flask application can be extended to include automation and scheduling features. This can involve scheduling the web scraping task at a regular interval or in response to a specific trigger, ensuring that the data is regularly updated.\n",
    "\n",
    "4. During development phase of a web scraping project, flask's light eight and easy to use nature can add in rapidly prototyping and testing various scraping functionalities.\n",
    "\n",
    "5. Flask offers flexibiliy and customisation options, that allow developers to create custom functionalities around the web scraping process such as implementing authentication mechanism, handeling different type of requests, managing sessions, or incorporating caching mechanism. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90af6ea-1b98-4f5e-ba9e-514f503553bc",
   "metadata": {},
   "source": [
    "# Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c838a7-5a3f-47f6-bd08-6eb690fd0fb6",
   "metadata": {},
   "source": [
    "**1. AWS CodePipeline:**\n",
    "\n",
    "AWS codePipeline is a fully managed continuous integration, continuous delivery(CI/CD) system that automates the process of building, testing and deploying an application. It allows developers to create and manage pipelines for software delivery workflows, enabling them to automate the steps involved in software release process. \n",
    "\n",
    "Some key features of AWS codePipeline include:\n",
    "\n",
    "1. codePipeline facilitates the creation of automated workflows, also known as pipelines where different stages(such as source code retrival, build, test and deployment) are connected in parallel to perform tasks sequentially or in parallel.\n",
    "\n",
    "2. It integrates with other AWS services such as AWS CodeBuild, AWS CodeDeploy, AWS CodeCommit and other third-party tools, allowing for a flexible and customizable workflow to fit specific workflow requirements.\n",
    "\n",
    "**2. AWS Elastic Beanstalk:**\n",
    "\n",
    "AWS Elastic Beanstalk is a Platform as a service(PaaS) offering that simplifies the deployment of the application in AWS cloud. It enables developers to quickly deploy web applications and services, without worrying about underlying infrastructure.\n",
    "\n",
    "Some Key Features of AWS Elastic Beanstalk includes:\n",
    "\n",
    "1. Elastic Beanstalk simplifies the deployment process by allowing he developers to upload their application code, and it automatically handles the deployment, load balancing, scailing and health monitoring of the application.\n",
    "\n",
    "2. It supports multiple programming languages like, Java, Python, Node.js, Ruby, .Net, PHP and Docker, allowing developers to deploy applications in different environment.\n",
    "\n",
    "3. Elastic Beanstalk can automatically scale the application, based on traffic fluctuations and provide monitoring and management capabilities, enabling easy scaling up or down to handle varying workloads."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
